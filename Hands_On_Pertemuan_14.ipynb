{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e2a030e3",
      "metadata": {
        "id": "e2a030e3"
      },
      "source": [
        "# Hands-On Pertemuan 14: Advanced Machine Learning using Spark MLlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "099562db",
      "metadata": {
        "id": "099562db"
      },
      "source": [
        "## Objectives:\n",
        "- Understand and implement advanced machine learning tasks using Spark MLlib.\n",
        "- Build and evaluate models using real-world datasets.\n",
        "- Explore techniques like feature engineering and hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77df771a",
      "metadata": {
        "id": "77df771a"
      },
      "source": [
        "## Introduction to Spark MLlib\n",
        "Spark MLlib is a scalable library for machine learning that integrates seamlessly with the Spark ecosystem. It supports a wide range of tasks, including regression, classification, clustering, and collaborative filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d9ae225b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9ae225b",
        "outputId": "bec0a091-7324-4a20-c134-7e6bd8a1e033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, [2.0, 3.0], 0), (2, [1.0, 5.0], 1), (3, [2.5, 4.5], 1), (4, [3.0, 6.0], 0)]\n",
        "columns = ['ID', 'Features', 'Label']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define a UDF to convert the array to a DenseVector\n",
        "@udf(returnType=VectorUDT())\n",
        "def to_vector(arr):\n",
        "    return Vectors.dense(arr)\n",
        "\n",
        "# Apply the UDF to the 'Features' column\n",
        "df = df.withColumn('Features', to_vector(df['Features']))\n",
        "\n",
        "# Create a VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['Features'], outputCol='Features_vec')\n",
        "\n",
        "# Transform the DataFrame\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Select the relevant columns and rename 'Features_vec' to 'Features'\n",
        "df = df.select('ID', 'Features_vec', 'Label').withColumnRenamed('Features_vec', 'Features')\n",
        "\n",
        "# Train the logistic regression model\n",
        "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
        "model = lr.fit(df)\n",
        "\n",
        "# Display coefficients and summary\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auecB57Po2xy",
        "outputId": "1efffac6-a154-4580-c663-88537212d8d2"
      },
      "id": "auecB57Po2xy",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-12.262057929180484,4.087352266486688]\n",
            "Intercept: 11.56891272665312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, [1.0, 1.0]), (2, [5.0, 5.0]), (3, [10.0, 10.0]), (4, [15.0, 15.0])]\n",
        "columns = ['ID', 'Features']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Define a UDF to convert the array to a DenseVector\n",
        "@udf(returnType=VectorUDT())\n",
        "def to_vector(arr):\n",
        "    return Vectors.dense(arr)\n",
        "\n",
        "# Apply the UDF to the 'Features' column\n",
        "df = df.withColumn('Features', to_vector(df['Features']))\n",
        "\n",
        "# Create a VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=['Features'], outputCol='Features_vec')\n",
        "\n",
        "# Transform the DataFrame\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Select the relevant columns and rename 'Features_vec' to 'Features'\n",
        "df = df.select('ID', 'Features_vec').withColumnRenamed('Features_vec', 'Features')\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='Features', k=2)\n",
        "model = kmeans.fit(df)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNVY1zE1pjTE",
        "outputId": "ec8b3018-c76c-4629-d39f-abd933d94b2b"
      },
      "id": "wNVY1zE1pjTE",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60a8d7e",
      "metadata": {
        "id": "a60a8d7e"
      },
      "source": [
        "## Homework\n",
        "- Load a real-world dataset into Spark and prepare it for machine learning tasks.\n",
        "- Build a classification model using Spark MLlib and evaluate its performance.\n",
        "- Explore hyperparameter tuning using cross-validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kOHe6tX0Y5g",
        "outputId": "c4bc829f-1695-4ffe-99c8-8e5b0fedf857"
      },
      "id": "5kOHe6tX0Y5g",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.sql.functions import col, when, regexp_replace\n",
        "from pyspark.sql.types import DoubleType, IntegerType, StructType, StructField, StringType\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName('Anime Analysis') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Define schema untuk memastikan pembacaan data yang benar\n",
        "schema = StructType([\n",
        "    StructField(\"Rank\", IntegerType(), True),\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Japanese_name\", StringType(), True),\n",
        "    StructField(\"Type\", StringType(), True),\n",
        "    StructField(\"Episodes\", StringType(), True),\n",
        "    StructField(\"Studio\", StringType(), True),\n",
        "    StructField(\"Release_season\", StringType(), True),\n",
        "    StructField(\"Tags\", StringType(), True),\n",
        "    StructField(\"Rating\", DoubleType(), True),\n",
        "    StructField(\"Release_year\", IntegerType(), True),\n",
        "    StructField(\"End_year\", StringType(), True),\n",
        "    StructField(\"Description\", StringType(), True),\n",
        "    StructField(\"Content_Warning\", StringType(), True),\n",
        "    StructField(\"Related_Mange\", StringType(), True),\n",
        "    StructField(\"Related_anime\", StringType(), True),\n",
        "    StructField(\"Voice_actors\", StringType(), True),\n",
        "    StructField(\"staff\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Baca CSV dengan parameter yang disesuaikan\n",
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"multiline\", \"true\") \\\n",
        "    .option(\"escape\", \"\\\"\") \\\n",
        "    .option(\"quote\", \"\\\"\") \\\n",
        "    .option(\"sep\", \",\") \\\n",
        "    .schema(schema) \\\n",
        "    .load('/content/drive/MyDrive/Bigdata_Dataset/Anime.csv')\n",
        "\n",
        "# Bersihkan dan persiapkan data\n",
        "df_cleaned = df.select(\n",
        "    col(\"Rank\"),\n",
        "    col(\"Type\"),\n",
        "    col(\"Episodes\").cast(IntegerType()),\n",
        "    col(\"Release_season\"),\n",
        "    col(\"Rating\"),\n",
        "    col(\"Release_year\"),\n",
        "    col(\"End_year\").cast(IntegerType())\n",
        ")\n",
        "\n",
        "# Handle null values\n",
        "df_cleaned = df_cleaned.na.fill({\n",
        "    \"Episodes\": 0,\n",
        "    \"Rating\": 0.0,\n",
        "    \"End_year\": 0,\n",
        "    \"Type\": \"Unknown\",\n",
        "    \"Release_season\": \"Unknown\"\n",
        "})\n",
        "\n",
        "# Convert categorical variables\n",
        "categorical_cols = [\"Type\", \"Release_season\"]\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\")\n",
        "           for col in categorical_cols]\n",
        "\n",
        "# Apply indexers\n",
        "for indexer in indexers:\n",
        "    df_cleaned = indexer.fit(df_cleaned).transform(df_cleaned)\n",
        "\n",
        "# Prepare feature vector\n",
        "numeric_cols = [\"Rank\", \"Episodes\", \"Rating\", \"Release_year\", \"End_year\"]\n",
        "feature_cols = numeric_cols + [col+\"_index\" for col in categorical_cols]\n",
        "\n",
        "# Create vector assembler\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "\n",
        "# Transform the data\n",
        "final_data = assembler.transform(df_cleaned)\n",
        "\n",
        "# Show the prepared dataset\n",
        "print(\"Schema of prepared dataset:\")\n",
        "final_data.printSchema()\n",
        "print(\"\\nSample of prepared dataset:\")\n",
        "final_data.show(5, truncate=False)\n",
        "\n",
        "# Save the prepared dataset if needed\n",
        "# final_data.write.mode(\"overwrite\").parquet(\"/path/to/save/prepared_anime_data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkqQoNJp04h3",
        "outputId": "fc7c6e86-73be-471a-d60d-f82a07499b11"
      },
      "id": "pkqQoNJp04h3",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema of prepared dataset:\n",
            "root\n",
            " |-- Rank: integer (nullable = true)\n",
            " |-- Type: string (nullable = false)\n",
            " |-- Episodes: integer (nullable = false)\n",
            " |-- Release_season: string (nullable = false)\n",
            " |-- Rating: double (nullable = false)\n",
            " |-- Release_year: integer (nullable = true)\n",
            " |-- End_year: integer (nullable = false)\n",
            " |-- Type_index: double (nullable = false)\n",
            " |-- Release_season_index: double (nullable = false)\n",
            " |-- features: vector (nullable = true)\n",
            "\n",
            "\n",
            "Sample of prepared dataset:\n",
            "+----+-----+--------+--------------+------+------------+--------+----------+--------------------+----------------------------------+\n",
            "|Rank|Type |Episodes|Release_season|Rating|Release_year|End_year|Type_index|Release_season_index|features                          |\n",
            "+----+-----+--------+--------------+------+------------+--------+----------+--------------------+----------------------------------+\n",
            "|1   |TV   |0       |Fall          |4.6   |NULL        |0       |0.0       |2.0                 |[1.0,0.0,4.6,NaN,0.0,0.0,2.0]     |\n",
            "|2   |TV   |13      |Spring        |4.6   |NULL        |0       |0.0       |1.0                 |[2.0,13.0,4.6,NaN,0.0,0.0,1.0]    |\n",
            "|3   |Web  |12      |Unknown       |4.58  |NULL        |0       |2.0       |0.0                 |[3.0,12.0,4.58,NaN,0.0,2.0,0.0]   |\n",
            "|4   |TV   |64      |Spring        |4.58  |NULL        |2010    |0.0       |1.0                 |[4.0,64.0,4.58,NaN,2010.0,0.0,1.0]|\n",
            "|5   |TV   |10      |Spring        |4.57  |NULL        |0       |0.0       |1.0                 |[5.0,10.0,4.57,NaN,0.0,0.0,1.0]   |\n",
            "+----+-----+--------+--------------+------+------------+--------+----------+--------------------+----------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_data.write.mode(\"overwrite\").parquet(\"/content/drive/MyDrive/Bigdata_Dataset/prepared_anime_data\")"
      ],
      "metadata": {
        "id": "9CwTV2Jp8nEp"
      },
      "id": "9CwTV2Jp8nEp",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql.functions import col, when, count\n",
        "from pyspark.sql.types import DoubleType, StructType, StructField\n",
        "\n",
        "def train_rating_classifier(df_cleaned):\n",
        "    \"\"\"\n",
        "    Train classifier to predict rating category based on features\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Check initial data\n",
        "        print(\"\\nInitial data count:\", df_cleaned.count())\n",
        "\n",
        "        # 2. Check for null values\n",
        "        print(\"\\nNull values in each column:\")\n",
        "        df_cleaned.select([count(when(col(c).isNull(), c)).alias(c)\n",
        "                         for c in df_cleaned.columns]).show()\n",
        "\n",
        "        # 3. Prepare features and label\n",
        "        feature_cols = [\"Episodes\", \"Release_year\", \"Release_season_index\"]\n",
        "\n",
        "        # Create rating categories and handle nulls\n",
        "        df_ml = df_cleaned.withColumn(\n",
        "            \"rating_category\",\n",
        "            when(col(\"Rating\").isNull(), None)\n",
        "            .when(col(\"Rating\") < 2.5, 0.0)\n",
        "            .when((col(\"Rating\") >= 2.5) & (col(\"Rating\") < 3.5), 1.0)\n",
        "            .when((col(\"Rating\") >= 3.5) & (col(\"Rating\") < 4.0), 2.0)\n",
        "            .otherwise(3.0)\n",
        "        )\n",
        "\n",
        "        # Select and cast columns, handle nulls\n",
        "        df_ml = df_ml.select(\n",
        "            col(\"rating_category\").cast(\"double\"),\n",
        "            *[when(col(c).isNull(), 0.0).otherwise(col(c)).cast(\"double\").alias(c)\n",
        "              for c in feature_cols]\n",
        "        ).na.drop()  # Remove any remaining null values\n",
        "\n",
        "        # 4. Check processed data\n",
        "        print(\"\\nProcessed data count:\", df_ml.count())\n",
        "        print(\"\\nRating category distribution:\")\n",
        "        df_ml.groupBy(\"rating_category\").count().orderBy(\"rating_category\").show()\n",
        "\n",
        "        # 5. Create feature vector\n",
        "        assembler = VectorAssembler(\n",
        "            inputCols=feature_cols,\n",
        "            outputCol=\"features\",\n",
        "            handleInvalid=\"skip\"\n",
        "        )\n",
        "        vector_data = assembler.transform(df_ml)\n",
        "\n",
        "        # Check vector data\n",
        "        print(\"\\nVector data count:\", vector_data.count())\n",
        "\n",
        "        # 6. Split data\n",
        "        train_data, test_data = vector_data.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "        print(\"\\nTraining data count:\", train_data.count())\n",
        "        print(\"Test data count:\", test_data.count())\n",
        "\n",
        "        # 7. Train model\n",
        "        rf = RandomForestClassifier(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"rating_category\",\n",
        "            numTrees=20,\n",
        "            maxDepth=7\n",
        "        )\n",
        "\n",
        "        model = rf.fit(train_data)\n",
        "\n",
        "        # 8. Evaluate model\n",
        "        predictions = model.transform(test_data)\n",
        "        evaluator = MulticlassClassificationEvaluator(\n",
        "            labelCol=\"rating_category\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"accuracy\"\n",
        "        )\n",
        "        accuracy = evaluator.evaluate(predictions)\n",
        "        print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        return model, assembler\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in training: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "# Test the model\n",
        "try:\n",
        "    # First, let's check the data\n",
        "    print(\"Original data overview:\")\n",
        "    df_cleaned.select(\"Rating\", \"Episodes\", \"Release_year\", \"Release_season_index\").show(5)\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining model...\")\n",
        "    model, assembler = train_rating_classifier(df_cleaned)\n",
        "\n",
        "    if model and assembler:\n",
        "        # Create sample new anime data\n",
        "        sample_data = [\n",
        "            (12.0, 2024.0, 1.0),\n",
        "            (24.0, 2024.0, 2.0),\n",
        "            (13.0, 2024.0, 3.0),\n",
        "            (50.0, 2024.0, 4.0)\n",
        "        ]\n",
        "\n",
        "        schema = StructType([\n",
        "            StructField(\"Episodes\", DoubleType(), True),\n",
        "            StructField(\"Release_year\", DoubleType(), True),\n",
        "            StructField(\"Release_season_index\", DoubleType(), True)\n",
        "        ])\n",
        "\n",
        "        new_anime = spark.createDataFrame(sample_data, schema=schema)\n",
        "\n",
        "        print(\"\\nNew Anime Data:\")\n",
        "        new_anime.show()\n",
        "\n",
        "        # Make predictions\n",
        "        vector_data = assembler.transform(new_anime)\n",
        "        predictions = model.transform(vector_data)\n",
        "\n",
        "        # Format predictions\n",
        "        final_predictions = predictions.select(\n",
        "            \"Episodes\",\n",
        "            \"Release_year\",\n",
        "            \"Release_season_index\",\n",
        "            when(col(\"prediction\") == 0.0, \"Poor (< 2.5)\")\n",
        "            .when(col(\"prediction\") == 1.0, \"Average (2.5-3.5)\")\n",
        "            .when(col(\"prediction\") == 2.0, \"Good (3.5-4.0)\")\n",
        "            .otherwise(\"Excellent (> 4.0)\")\n",
        "            .alias(\"Predicted_Rating_Category\")\n",
        "        )\n",
        "\n",
        "        print(\"\\nPredictions for New Anime:\")\n",
        "        final_predictions.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKUgBA_tKe8v",
        "outputId": "a49b5c1e-794c-4fc6-d1d1-576c8fefaeb7"
      },
      "id": "wKUgBA_tKe8v",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data overview:\n",
            "+------+--------+------------+--------------------+\n",
            "|Rating|Episodes|Release_year|Release_season_index|\n",
            "+------+--------+------------+--------------------+\n",
            "|   4.6|       0|        NULL|                 2.0|\n",
            "|   4.6|      13|        NULL|                 1.0|\n",
            "|  4.58|      12|        NULL|                 0.0|\n",
            "|  4.58|      64|        NULL|                 1.0|\n",
            "|  4.57|      10|        NULL|                 1.0|\n",
            "+------+--------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Training model...\n",
            "\n",
            "Initial data count: 18495\n",
            "\n",
            "Null values in each column:\n",
            "+----+----+--------+--------------+------+------------+--------+----------+--------------------+\n",
            "|Rank|Type|Episodes|Release_season|Rating|Release_year|End_year|Type_index|Release_season_index|\n",
            "+----+----+--------+--------------+------+------------+--------+----------+--------------------+\n",
            "|   0|   0|       0|             0|     0|       18495|       0|         0|                   0|\n",
            "+----+----+--------+--------------+------+------------+--------+----------+--------------------+\n",
            "\n",
            "\n",
            "Processed data count: 18495\n",
            "\n",
            "Rating category distribution:\n",
            "+---------------+-----+\n",
            "|rating_category|count|\n",
            "+---------------+-----+\n",
            "|            0.0| 3484|\n",
            "|            1.0| 9813|\n",
            "|            2.0| 4298|\n",
            "|            3.0|  900|\n",
            "+---------------+-----+\n",
            "\n",
            "\n",
            "Vector data count: 18495\n",
            "\n",
            "Training data count: 13020\n",
            "Test data count: 5475\n",
            "\n",
            "Model Accuracy: 0.5302\n",
            "\n",
            "New Anime Data:\n",
            "+--------+------------+--------------------+\n",
            "|Episodes|Release_year|Release_season_index|\n",
            "+--------+------------+--------------------+\n",
            "|    12.0|      2024.0|                 1.0|\n",
            "|    24.0|      2024.0|                 2.0|\n",
            "|    13.0|      2024.0|                 3.0|\n",
            "|    50.0|      2024.0|                 4.0|\n",
            "+--------+------------+--------------------+\n",
            "\n",
            "\n",
            "Predictions for New Anime:\n",
            "+--------+------------+--------------------+-------------------------+\n",
            "|Episodes|Release_year|Release_season_index|Predicted_Rating_Category|\n",
            "+--------+------------+--------------------+-------------------------+\n",
            "|    12.0|      2024.0|                 1.0|        Average (2.5-3.5)|\n",
            "|    24.0|      2024.0|                 2.0|        Average (2.5-3.5)|\n",
            "|    13.0|      2024.0|                 3.0|        Average (2.5-3.5)|\n",
            "|    50.0|      2024.0|                 4.0|        Average (2.5-3.5)|\n",
            "+--------+------------+--------------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "def tune_hyperparameters(df_cleaned, assembler):\n",
        "    \"\"\"\n",
        "    Perform hyperparameter tuning using cross-validation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Prepare data\n",
        "        print(\"\\nPreparing data for tuning...\")\n",
        "\n",
        "        # Select necessary columns and handle nulls\n",
        "        feature_cols = [\"Episodes\", \"Release_year\", \"Release_season_index\"]\n",
        "        df_ml = df_cleaned.select(\n",
        "            \"Rating\",\n",
        "            *feature_cols\n",
        "        ).na.fill(0)  # Fill nulls with 0\n",
        "\n",
        "        # Create rating categories\n",
        "        df_ml = df_ml.withColumn(\n",
        "            \"rating_category\",\n",
        "            when(col(\"Rating\") < 2.5, 0.0)\n",
        "            .when((col(\"Rating\") >= 2.5) & (col(\"Rating\") < 3.5), 1.0)\n",
        "            .when((col(\"Rating\") >= 3.5) & (col(\"Rating\") < 4.0), 2.0)\n",
        "            .otherwise(3.0)\n",
        "        )\n",
        "\n",
        "        # Check data after transformation\n",
        "        print(f\"\\nData count after category transformation: {df_ml.count()}\")\n",
        "        print(\"\\nRating category distribution:\")\n",
        "        df_ml.groupBy(\"rating_category\").count().orderBy(\"rating_category\").show()\n",
        "\n",
        "        # Create feature vector\n",
        "        vector_data = assembler.transform(df_ml)\n",
        "        print(f\"\\nVector data count: {vector_data.count()}\")\n",
        "\n",
        "        # Drop any rows with null features or labels\n",
        "        vector_data = vector_data.na.drop(subset=[\"features\", \"rating_category\"])\n",
        "        print(f\"Final vector data count: {vector_data.count()}\")\n",
        "\n",
        "        if vector_data.count() == 0:\n",
        "            raise Exception(\"Dataset is empty after preprocessing\")\n",
        "\n",
        "        # 2. Create new RandomForestClassifier\n",
        "        rf = RandomForestClassifier(\n",
        "            featuresCol=\"features\",\n",
        "            labelCol=\"rating_category\",\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        # 3. Create parameter grid\n",
        "        paramGrid = ParamGridBuilder() \\\n",
        "            .addGrid(rf.numTrees, [10, 20]) \\\n",
        "            .addGrid(rf.maxDepth, [5, 7]) \\\n",
        "            .addGrid(rf.minInstancesPerNode, [1, 2]) \\\n",
        "            .build()\n",
        "\n",
        "        # 4. Create evaluator\n",
        "        evaluator = MulticlassClassificationEvaluator(\n",
        "            labelCol=\"rating_category\",\n",
        "            predictionCol=\"prediction\",\n",
        "            metricName=\"accuracy\"\n",
        "        )\n",
        "\n",
        "        # 5. Split data\n",
        "        train_data, test_data = vector_data.randomSplit([0.7, 0.3], seed=42)\n",
        "        print(f\"\\nTraining data count: {train_data.count()}\")\n",
        "        print(f\"Test data count: {test_data.count()}\")\n",
        "\n",
        "        if train_data.count() == 0 or test_data.count() == 0:\n",
        "            raise Exception(\"Training or test data is empty after split\")\n",
        "\n",
        "        # 6. Create and fit CrossValidator\n",
        "        crossval = CrossValidator(\n",
        "            estimator=rf,\n",
        "            estimatorParamMaps=paramGrid,\n",
        "            evaluator=evaluator,\n",
        "            numFolds=3,\n",
        "            seed=42\n",
        "        )\n",
        "\n",
        "        print(\"\\nStarting cross-validation...\")\n",
        "        print(\"This may take a while as it tests multiple parameter combinations.\")\n",
        "        cvModel = crossval.fit(train_data)\n",
        "\n",
        "        # 7. Get and evaluate best model\n",
        "        best_model = cvModel.bestModel\n",
        "        predictions = best_model.transform(test_data)\n",
        "\n",
        "        # 8. Print results\n",
        "        print(\"\\nBest Model Parameters:\")\n",
        "        print(f\"Number of Trees: {best_model.numTrees}\")\n",
        "        print(f\"Max Depth: {best_model.maxDepth}\")\n",
        "        print(f\"Min Instances Per Node: {best_model.minInstancesPerNode}\")\n",
        "\n",
        "        print(\"\\nBest Model Performance:\")\n",
        "        metrics = {\n",
        "            \"accuracy\": \"accuracy\",\n",
        "            \"f1\": \"f1\",\n",
        "            \"precision\": \"weightedPrecision\",\n",
        "            \"recall\": \"weightedRecall\"\n",
        "        }\n",
        "\n",
        "        for metric_name, metric_value in metrics.items():\n",
        "            evaluator.setMetricName(metric_value)\n",
        "            score = evaluator.evaluate(predictions)\n",
        "            print(f\"{metric_name.title()}: {score:.4f}\")\n",
        "\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        predictions.groupBy(\"rating_category\", \"prediction\") \\\n",
        "            .count() \\\n",
        "            .orderBy(\"rating_category\", \"prediction\") \\\n",
        "            .show()\n",
        "\n",
        "        return best_model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in hyperparameter tuning: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Test the tuning\n",
        "try:\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "\n",
        "    # Check if df_cleaned has data\n",
        "    print(f\"\\nInitial data count: {df_cleaned.count()}\")\n",
        "    print(\"\\nSample of initial data:\")\n",
        "    df_cleaned.select(\"Rating\", \"Episodes\", \"Release_year\", \"Release_season_index\").show(5)\n",
        "\n",
        "    best_model = tune_hyperparameters(df_cleaned, assembler)\n",
        "\n",
        "    if best_model:\n",
        "        print(\"\\nHyperparameter tuning completed successfully!\")\n",
        "\n",
        "        # Test the best model with sample data\n",
        "        sample_data = [\n",
        "            (12.0, 2024.0, 1.0),\n",
        "            (24.0, 2024.0, 2.0),\n",
        "            (13.0, 2024.0, 3.0),\n",
        "            (50.0, 2024.0, 4.0)\n",
        "        ]\n",
        "\n",
        "        schema = StructType([\n",
        "            StructField(\"Episodes\", DoubleType(), True),\n",
        "            StructField(\"Release_year\", DoubleType(), True),\n",
        "            StructField(\"Release_season_index\", DoubleType(), True)\n",
        "        ])\n",
        "\n",
        "        new_anime = spark.createDataFrame(sample_data, schema=schema)\n",
        "        vector_data = assembler.transform(new_anime)\n",
        "\n",
        "        predictions = best_model.transform(vector_data)\n",
        "        final_predictions = predictions.select(\n",
        "            \"Episodes\",\n",
        "            \"Release_year\",\n",
        "            \"Release_season_index\",\n",
        "            when(col(\"prediction\") == 0.0, \"Poor (< 2.5)\")\n",
        "            .when(col(\"prediction\") == 1.0, \"Average (2.5-3.5)\")\n",
        "            .when(col(\"prediction\") == 2.0, \"Good (3.5-4.0)\")\n",
        "            .otherwise(\"Excellent (> 4.0)\")\n",
        "            .alias(\"Predicted_Rating_Category\")\n",
        "        )\n",
        "\n",
        "        print(\"\\nPredictions with Best Model:\")\n",
        "        final_predictions.show()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c2uZ-GcOzVn",
        "outputId": "9792ba14-62ea-4cd5-afe9-2648adc0242e"
      },
      "id": "5c2uZ-GcOzVn",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter tuning...\n",
            "\n",
            "Initial data count: 18495\n",
            "\n",
            "Sample of initial data:\n",
            "+------+--------+------------+--------------------+\n",
            "|Rating|Episodes|Release_year|Release_season_index|\n",
            "+------+--------+------------+--------------------+\n",
            "|   4.6|       0|        NULL|                 2.0|\n",
            "|   4.6|      13|        NULL|                 1.0|\n",
            "|  4.58|      12|        NULL|                 0.0|\n",
            "|  4.58|      64|        NULL|                 1.0|\n",
            "|  4.57|      10|        NULL|                 1.0|\n",
            "+------+--------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Preparing data for tuning...\n",
            "\n",
            "Data count after category transformation: 18495\n",
            "\n",
            "Rating category distribution:\n",
            "+---------------+-----+\n",
            "|rating_category|count|\n",
            "+---------------+-----+\n",
            "|            0.0| 3484|\n",
            "|            1.0| 9813|\n",
            "|            2.0| 4298|\n",
            "|            3.0|  900|\n",
            "+---------------+-----+\n",
            "\n",
            "\n",
            "Vector data count: 18495\n",
            "Final vector data count: 18495\n",
            "\n",
            "Training data count: 13020\n",
            "Test data count: 5475\n",
            "\n",
            "Starting cross-validation...\n",
            "This may take a while as it tests multiple parameter combinations.\n",
            "\n",
            "Best Model Parameters:\n",
            "Number of Trees: RandomForestClassifier_3c365790c03e__numTrees\n",
            "Max Depth: RandomForestClassifier_3c365790c03e__maxDepth\n",
            "Min Instances Per Node: RandomForestClassifier_3c365790c03e__minInstancesPerNode\n",
            "\n",
            "Best Model Performance:\n",
            "Accuracy: 0.5255\n",
            "F1: 0.3800\n",
            "Precision: 0.4386\n",
            "Recall: 0.5255\n",
            "\n",
            "Confusion Matrix:\n",
            "+---------------+----------+-----+\n",
            "|rating_category|prediction|count|\n",
            "+---------------+----------+-----+\n",
            "|            0.0|       0.0|   14|\n",
            "|            0.0|       1.0|  975|\n",
            "|            0.0|       2.0|    1|\n",
            "|            1.0|       0.0|   11|\n",
            "|            1.0|       1.0| 2829|\n",
            "|            1.0|       2.0|   55|\n",
            "|            2.0|       0.0|    4|\n",
            "|            2.0|       1.0| 1291|\n",
            "|            2.0|       2.0|   34|\n",
            "|            3.0|       0.0|    1|\n",
            "|            3.0|       1.0|  238|\n",
            "|            3.0|       2.0|   22|\n",
            "+---------------+----------+-----+\n",
            "\n",
            "\n",
            "Hyperparameter tuning completed successfully!\n",
            "\n",
            "Predictions with Best Model:\n",
            "+--------+------------+--------------------+-------------------------+\n",
            "|Episodes|Release_year|Release_season_index|Predicted_Rating_Category|\n",
            "+--------+------------+--------------------+-------------------------+\n",
            "|    12.0|      2024.0|                 1.0|        Average (2.5-3.5)|\n",
            "|    24.0|      2024.0|                 2.0|           Good (3.5-4.0)|\n",
            "|    13.0|      2024.0|                 3.0|        Average (2.5-3.5)|\n",
            "|    50.0|      2024.0|                 4.0|        Average (2.5-3.5)|\n",
            "+--------+------------+--------------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}