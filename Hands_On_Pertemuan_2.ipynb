{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac766966",
   "metadata": {},
   "source": [
    "# Hands-On Pertemuan 2: Instalasi dan Konfigurasi Hadoop serta Struktur HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c2d52",
   "metadata": {},
   "source": [
    "## Tujuan:\n",
    "- Memahami langkah-langkah instalasi dan konfigurasi Hadoop.\n",
    "- Mempraktikkan bagaimana menggunakan Hadoop dan memahami struktur HDFS.\n",
    "- Mengeksplorasi command line interface (CLI) Hadoop dan melakukan operasi dasar pada HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7a499",
   "metadata": {},
   "source": [
    "### 1. Instalasi Hadoop di Mode Standalone\n",
    "1. **Unduh Hadoop**: Kunjungi [Apache Hadoop](https://hadoop.apache.org/releases.html) untuk mengunduh versi terbaru.\n",
    "2. **Ekstrak dan Setup**: Tambahkan Hadoop ke dalam `$PATH` dan konfigurasi environment variable.\n",
    "   ```bash\n",
    "   export HADOOP_HOME=/path/to/hadoop\n",
    "   export PATH=$PATH:$HADOOP_HOME/bin\n",
    "   export JAVA_HOME=/path/to/java\n",
    "   ```\n",
    "3. **Format HDFS**: Format Hadoop file system dengan:\n",
    "   ```bash\n",
    "   hdfs namenode -format\n",
    "   ```\n",
    "4. **Start Hadoop**: Jalankan Hadoop dengan perintah:\n",
    "   ```bash\n",
    "   start-dfs.sh\n",
    "   ```\n",
    "- **Tugas 1**: Format HDFS dan jalankan dalam mode standalone. Verifikasi dengan menjalankan perintah `hadoop version`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc92ea4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "C:\\Users\\byu>hadoop version\n",
    "Hadoop 3.3.6\n",
    "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
    "Compiled by ubuntu on 2023-06-18T08:22Z\n",
    "Compiled on platform linux-x86_64\n",
    "Compiled with protoc 3.7.1\n",
    "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
    "This command was run using /C:/hadoop/hadoop-3.3.6/share/hadoop/common/hadoop-common-3.3.6.jar\n",
    "\n",
    "C:\\Users\\byu>start-dfs.cmd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5ffee",
   "metadata": {},
   "source": [
    "### 2. Struktur HDFS dan Operasi Dasar\n",
    "HDFS merupakan file system terdistribusi yang memungkinkan penyimpanan dan pemrosesan data besar secara paralel.\n",
    "- **Operasi Dasar HDFS**:\n",
    "   - Buat direktori baru di HDFS:\n",
    "   ```bash\n",
    "   hdfs dfs -mkdir /user/student\n",
    "   ```\n",
    "   - Unggah file ke HDFS:\n",
    "   ```bash\n",
    "   hdfs dfs -put input.txt /user/student/\n",
    "   ```\n",
    "   - Tampilkan file yang telah diunggah:\n",
    "   ```bash\n",
    "   hdfs dfs -ls /user/student/\n",
    "   ```\n",
    "- **Tugas 2**: Buat direktori di HDFS, upload file teks, tampilkan konten file, dan hapus file tersebut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07e77b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "C:\\Users\\byu>hdfs dfs -mkdir /user\n",
    "\n",
    "C:\\Users\\byu>hdfs dfs -mkdir /user/mibyu\n",
    "\n",
    "C:\\Users\\byu>echo \"Djancuk\" > byu.txt\n",
    "\n",
    "C:\\Users\\byu>hdfs dfs -put byu.txt /user/mibyu/\n",
    "\n",
    "C:\\Users\\byu>hdfs dfs -ls /user/mibyu/\n",
    "Found 1 items\n",
    "-rw-r--r--   1 byu supergroup         12 2024-09-03 21:23 /user/mibyu/byu.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f155c9b",
   "metadata": {},
   "source": [
    "### 3. Operasi File di HDFS\n",
    "Lakukan operasi pada file yang telah diunggah:\n",
    "1. **Melihat Konten File**:\n",
    "   ```bash\n",
    "   hdfs dfs -cat /user/student/input.txt\n",
    "   ```\n",
    "2. **Menduplikasi File**:\n",
    "   ```bash\n",
    "   hdfs dfs -cp /user/student/input.txt /user/student/input_copy.txt\n",
    "   ```\n",
    "3. **Menghapus File dari HDFS**:\n",
    "   ```bash\n",
    "   hdfs dfs -rm /user/student/input_copy.txt\n",
    "   ```\n",
    "- **Tugas 3**: Lakukan operasi untuk menampilkan konten file, menduplikasi, dan menghapus file di HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf37ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "C:\\Users\\byu>hdfs dfs -cat byu.txt\n",
    "cat: `byu.txt': No such file or directory\n",
    "\n",
    "C:\\Users\\byu>hdfs dfs -cat /user/mibyu/byu.txt\n",
    "\"Djancuk\"\n",
    "\n",
    "C:\\Users\\byu>hdfs dfs -cp /user/mibyu/byu.txt /user/mibyu/byu01.txt\n",
    "\n",
    "C:\\Users\\byu>hdfs dfs -cat /user/mibyu/byu01.txt\n",
    "\"Djancuk\"\n",
    "\n",
    "C:\\Users\\byu>hdfs dfs -rm /user/mibyu/byu01.txt\n",
    "Deleted /user/mibyu/byu01.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c979e38",
   "metadata": {},
   "source": [
    "### 4. Menganalisis Struktur Penyimpanan di HDFS\n",
    "Untuk memahami bagaimana HDFS mengelola penyimpanan, gunakan perintah berikut:\n",
    "- **Menampilkan informasi penyimpanan HDFS**:\n",
    "   ```bash\n",
    "   hdfs dfsadmin -report\n",
    "   ```\n",
    "- **Menampilkan status block**:\n",
    "   ```bash\n",
    "   hdfs fsck / -files -blocks -locations\n",
    "   ```\n",
    "- **Tugas 4**: Lakukan analisis pada struktur penyimpanan di HDFS dan tuliskan laporan berdasarkan hasil dari `hdfs dfsadmin -report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad4505e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "C:\\Users\\byu>hdfs dfsadmin -report\n",
    "Configured Capacity: 510938574848 (475.85 GB)\n",
    "Present Capacity: 9155469655 (8.53 GB)\n",
    "DFS Remaining: 9155469312 (8.53 GB)\n",
    "DFS Used: 343 (343 B)\n",
    "DFS Used%: 0.00%\n",
    "Replicated Blocks:\n",
    "        Under replicated blocks: 0\n",
    "        Blocks with corrupt replicas: 0\n",
    "        Missing blocks: 0\n",
    "        Missing blocks (with replication factor 1): 0\n",
    "        Low redundancy blocks with highest priority to recover: 0\n",
    "        Pending deletion blocks: 0\n",
    "Erasure Coded Block Groups:\n",
    "        Low redundancy block groups: 0\n",
    "        Block groups with corrupt internal blocks: 0\n",
    "        Missing block groups: 0\n",
    "        Low redundancy blocks with highest priority to recover: 0\n",
    "        Pending deletion blocks: 0\n",
    "\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3d99e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "C:\\Users\\byu>hdfs fsck / -files -bloks -locations\n",
    "\n",
    "Status: HEALTHY\n",
    " Number of data-nodes:  1\n",
    " Number of racks:               1\n",
    " Total dirs:                    3\n",
    " Total symlinks:                0\n",
    "\n",
    "Replicated Blocks:\n",
    " Total size:    12 B\n",
    " Total files:   1\n",
    " Total blocks (validated):      1 (avg. block size 12 B)\n",
    " Minimally replicated blocks:   1 (100.0 %)\n",
    " Over-replicated blocks:        0 (0.0 %)\n",
    " Under-replicated blocks:       0 (0.0 %)\n",
    " Mis-replicated blocks:         0 (0.0 %)\n",
    " Default replication factor:    1\n",
    " Average block replication:     1.0\n",
    " Missing blocks:                0\n",
    " Corrupt blocks:                0\n",
    " Missing replicas:              0 (0.0 %)\n",
    " Blocks queued for replication: 0\n",
    "\n",
    "Erasure Coded Block Groups:\n",
    " Total size:    0 B\n",
    " Total files:   0\n",
    " Total block groups (validated):        0\n",
    " Minimally erasure-coded block groups:  0\n",
    " Over-erasure-coded block groups:       0\n",
    " Under-erasure-coded block groups:      0\n",
    " Unsatisfactory placement block groups: 0\n",
    " Average block group size:      0.0\n",
    " Missing block groups:          0\n",
    " Corrupt block groups:          0\n",
    " Missing internal blocks:       0\n",
    " Blocks queued for replication: 0\n",
    "FSCK ended at Tue Sep 03 21:39:21 WIB 2024 in 23 milliseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179e5ca0",
   "metadata": {},
   "source": [
    "### 5. Tugas Tambahan: Integrasi Hadoop dengan Spark\n",
    "- Coba instal Spark dan konfigurasi dengan Hadoop. Lakukan operasi sederhana untuk memproses data menggunakan Spark yang tersimpan di HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f9040",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import SparkSeassion\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tampilkan Isi File HDFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Membaca file teks dari HDFS menggunakan alamat IP\n",
    "data = spark.read.text(\"hdfs://127.0.0.1:9000/user/mibyu/byu.txt\")\n",
    "\n",
    "# Menampilkan isi file\n",
    "data.show(truncate=False)\n",
    "\n",
    "# Menutup Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
