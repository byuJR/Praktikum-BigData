{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f89f7684",
      "metadata": {
        "id": "f89f7684"
      },
      "source": [
        "# Hands-On Pertemuan 6: Data Processing dengan Apache Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30ce9d1",
      "metadata": {
        "id": "e30ce9d1"
      },
      "source": [
        "## Tujuan:\n",
        "- Memahami dan mempraktikkan data processing menggunakan Apache Spark.\n",
        "- Menggunakan Spark untuk operasi data yang efisien pada dataset besar.\n",
        "- Menerapkan teknik canggih dalam Spark untuk mengatasi kasus penggunaan nyata."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2x9IJM7K7Gw",
        "outputId": "a38a0f75-5598-48a3-ad28-fba2b23b59b3"
      },
      "id": "F2x9IJM7K7Gw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=2bcbba245ab37d4d75d2845328fd24914473c2ec11d283887f607f8856c282a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1TR6HvJK89O",
        "outputId": "2e63d231-2dda-460b-91af-2dfa49422d80"
      },
      "id": "h1TR6HvJK89O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HheDsJE-K-nR",
        "outputId": "3ed3aecb-9c95-4fca-a1ec-92940e8763ca"
      },
      "id": "HheDsJE-K-nR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5c2f90",
      "metadata": {
        "id": "cd5c2f90"
      },
      "source": [
        "### 1. Pengenalan Spark DataFrames\n",
        "Spark DataFrame menyediakan struktur data yang optimal dengan operasi yang dioptimalkan untuk pemrosesan data besar, yang sangat mirip dengan DataFrame di Pandas atau di RDBMS.\n",
        "\n",
        "- **Tugas 1**: Buat DataFrame sederhana di Spark dan eksplorasi beberapa fungsi dasar yang tersedia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986d01c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "986d01c7",
        "outputId": "04d7aafa-f83e-4de4-8127-dff48e05c459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+\n",
            "|EmployeeName|Department|Salary|\n",
            "+------------+----------+------+\n",
            "|       James|     Sales|  3000|\n",
            "|     Michael|     Sales|  4600|\n",
            "|      Robert|     Sales|  4100|\n",
            "|       Maria|   Finance|  3000|\n",
            "+------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Contoh membuat DataFrame sederhana dan operasi dasar\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('HandsOnPertemuan6').getOrCreate()\n",
        "\n",
        "data = [('James', 'Sales', 3000),\n",
        "        ('Michael', 'Sales', 4600),\n",
        "        ('Robert', 'Sales', 4100),\n",
        "        ('Maria', 'Finance', 3000)]\n",
        "columns = ['EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fca66b73",
      "metadata": {
        "id": "fca66b73"
      },
      "source": [
        "### 2. Transformasi Dasar dengan DataFrames\n",
        "Pemrosesan data meliputi transformasi seperti filtering, selections, dan aggregations. Spark menyediakan cara efisien untuk melaksanakan operasi ini.\n",
        "\n",
        "- **Tugas 2**: Gunakan operasi filter, select, groupBy untuk mengekstrak informasi dari data, serta lakukan agregasi data untuk mendapatkan insight tentang dataset menggunakan perintah seperti mean, max, sum."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter karyawan yang gajinya lebih dari 4000\n",
        "df_filtered = df.filter(df.Salary > 4000)\n",
        "df_filtered.show()\n",
        "\n",
        "# Select kolom EmployeeName dan Department\n",
        "df_selected = df.select('EmployeeName', 'Department')\n",
        "df_selected.show()\n",
        "\n",
        "# GroupBy Department dan hitung rata-rata gaji\n",
        "df_grouped = df.groupBy('Department').agg({'Salary': 'mean'})\n",
        "df_grouped.show()\n",
        "\n",
        "# Hitung jumlah karyawan di setiap departemen\n",
        "df_count = df.groupBy('Department').count()\n",
        "df_count.show()\n",
        "\n",
        "# Cari gaji maksimum di setiap departemen\n",
        "df_max_salary = df.groupBy('Department').agg({'Salary': 'max'})\n",
        "df_max_salary.show()\n",
        "\n",
        "# Hitung total gaji untuk setiap departemen\n",
        "df_total_salary = df.groupBy('Department').agg({'Salary': 'sum'})\n",
        "df_total_salary.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCaak0ecp3I1",
        "outputId": "b379ba17-0378-4e9a-b472-d9c00241124d"
      },
      "id": "oCaak0ecp3I1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+\n",
            "|EmployeeName|Department|Salary|\n",
            "+------------+----------+------+\n",
            "|     Michael|     Sales|  4600|\n",
            "|      Robert|     Sales|  4100|\n",
            "+------------+----------+------+\n",
            "\n",
            "+------------+----------+\n",
            "|EmployeeName|Department|\n",
            "+------------+----------+\n",
            "|       James|     Sales|\n",
            "|     Michael|     Sales|\n",
            "|      Robert|     Sales|\n",
            "|       Maria|   Finance|\n",
            "+------------+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3900.0|\n",
            "|   Finance|     3000.0|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----+\n",
            "|Department|count|\n",
            "+----------+-----+\n",
            "|     Sales|    3|\n",
            "|   Finance|    1|\n",
            "+----------+-----+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|max(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|       4600|\n",
            "|   Finance|       3000|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|sum(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|      11700|\n",
            "|   Finance|       3000|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58232678",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58232678",
        "outputId": "cd7ed429-a083-40b6-afd4-7e8a3a1e54a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------+\n",
            "|EmployeeName|Salary|\n",
            "+------------+------+\n",
            "|       James|  3000|\n",
            "|     Michael|  4600|\n",
            "|      Robert|  4100|\n",
            "|       Maria|  3000|\n",
            "+------------+------+\n",
            "\n",
            "+------------+----------+------+\n",
            "|EmployeeName|Department|Salary|\n",
            "+------------+----------+------+\n",
            "|     Michael|     Sales|  4600|\n",
            "|      Robert|     Sales|  4100|\n",
            "+------------+----------+------+\n",
            "\n",
            "+----------+-----------+\n",
            "|Department|avg(Salary)|\n",
            "+----------+-----------+\n",
            "|     Sales|     3900.0|\n",
            "|   Finance|     3000.0|\n",
            "+----------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Contoh operasi transformasi DataFrame\n",
        "df.select('EmployeeName', 'Salary').show()\n",
        "df.filter(df['Salary'] > 3000).show()\n",
        "df.groupBy('Department').avg('Salary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89763d36",
      "metadata": {
        "id": "89763d36"
      },
      "source": [
        "### 3. Bekerja dengan Tipe Data Kompleks\n",
        "Spark mendukung tipe data yang kompleks seperti maps, arrays, dan structs yang memungkinkan operasi yang lebih kompleks pada dataset yang kompleks.\n",
        "\n",
        "- **Tugas 3**: Eksplorasi bagaimana mengolah tipe data kompleks dalam Spark DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14701d79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14701d79",
        "outputId": "9c6f14df-ec27-4b47-a1aa-8f09592a0754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+-----------+\n",
            "|EmployeeName|Department|Salary|SalaryBonus|\n",
            "+------------+----------+------+-----------+\n",
            "|       James|     Sales|  3000|      300.0|\n",
            "|     Michael|     Sales|  4600|      460.0|\n",
            "|      Robert|     Sales|  4100|      410.0|\n",
            "|       Maria|   Finance|  3000|      300.0|\n",
            "+------------+----------+------+-----------+\n",
            "\n",
            "+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeName|Department|Salary|SalaryBonus|TotalCompensation|\n",
            "+------------+----------+------+-----------+-----------------+\n",
            "|       James|     Sales|  3000|      300.0|           3300.0|\n",
            "|     Michael|     Sales|  4600|      460.0|           5060.0|\n",
            "|      Robert|     Sales|  4100|      410.0|           4510.0|\n",
            "|       Maria|   Finance|  3000|      300.0|           3300.0|\n",
            "+------------+----------+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "df = df.withColumn('SalaryBonus', df['Salary'] * 0.1)  # Menyimpan SalaryBonus ke df\n",
        "df.show()\n",
        "df.withColumn('TotalCompensation', df['Salary'] + df['SalaryBonus']).show() # Menghitung dan menampilkan TotalCompensation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b3b58dd",
      "metadata": {
        "id": "5b3b58dd"
      },
      "source": [
        "### 4. Operasi Data Lanjutan\n",
        "Menggunakan Spark untuk operasi lanjutan seperti window functions, user-defined functions (UDFs), dan mengoptimalkan query.\n",
        "\n",
        "- **Tugas 4**: Implementasikan window function untuk menghitung running totals atau rangkings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, sum, col\n",
        "\n",
        "# Definisikan window specification\n",
        "windowSpec = Window.partitionBy(\"Department\").orderBy(\"Salary\")\n",
        "\n",
        "# Hitung running total gaji untuk setiap departemen\n",
        "df_running_total = df.withColumn(\"RunningTotal\", sum(\"Salary\").over(windowSpec))\n",
        "df_running_total.show()\n",
        "\n",
        "# Hitung ranking gaji untuk setiap departemen\n",
        "df_rank = df.withColumn(\"Rank\", row_number().over(windowSpec))\n",
        "df_rank.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHlOQNJRr5n4",
        "outputId": "026069c8-23b0-4225-8187-35e0ea06eaca"
      },
      "id": "kHlOQNJRr5n4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+------------+\n",
            "|EmployeeName|Department|Salary|RunningTotal|\n",
            "+------------+----------+------+------------+\n",
            "|       Maria|   Finance|  3000|        3000|\n",
            "|       James|     Sales|  3000|        3000|\n",
            "|      Robert|     Sales|  4100|        7100|\n",
            "|     Michael|     Sales|  4600|       11700|\n",
            "+------------+----------+------+------------+\n",
            "\n",
            "+------------+----------+------+----+\n",
            "|EmployeeName|Department|Salary|Rank|\n",
            "+------------+----------+------+----+\n",
            "|       Maria|   Finance|  3000|   1|\n",
            "|       James|     Sales|  3000|   1|\n",
            "|      Robert|     Sales|  4100|   2|\n",
            "|     Michael|     Sales|  4600|   3|\n",
            "+------------+----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "035312eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "035312eb",
        "outputId": "4a34dc4e-309c-4d68-b6ef-b14c9f58ed37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------+-----------+----+\n",
            "|EmployeeName|Department|Salary|SalaryBonus|Rank|\n",
            "+------------+----------+------+-----------+----+\n",
            "|       Maria|   Finance|  3000|      300.0|   1|\n",
            "|       James|     Sales|  3000|      300.0|   1|\n",
            "|      Robert|     Sales|  4100|      410.0|   2|\n",
            "|     Michael|     Sales|  4600|      460.0|   3|\n",
            "+------------+----------+------+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Contoh menggunakan window functions\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "windowSpec = Window.partitionBy('Department').orderBy('Salary')\n",
        "df.withColumn('Rank', F.rank().over(windowSpec)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8a097ec",
      "metadata": {
        "id": "f8a097ec"
      },
      "source": [
        "### 5. Kesimpulan dan Eksplorasi Lebih Lanjut\n",
        "Review apa yang telah dipelajari tentang pemrosesan data menggunakan Spark dan eksplorasi teknik lebih lanjut untuk mengoptimalkan pemrosesan data Anda.\n",
        "- **Tugas 5**: Buat ringkasan dari semua operasi yang telah dilakukan dan bagaimana teknik ini dapat diterapkan pada proyek data Anda."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}